{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9fa7317",
   "metadata": {},
   "source": [
    "# Wild Blueberry Yield Prediction\n",
    "\n",
    "**Context**: The dataset used for predictive modeling was generated by the Wild Blueberry Pollination Simulation Model, which is an open-source, spatially-explicit computer simulation program that enables exploration of how various factors, including plant spatial arrangement, outcrossing and self-pollination, bee species compositions and weather conditions, in isolation and combination, affect pollination efficiency and yield of the wild blueberry agroecosystem.\n",
    "\n",
    "**Goal**: Through the provided features predict the wild blueberry yield amount.\n",
    "\n",
    "**Feature Description**:\n",
    "- Clonesize: m2 The average blueberry clone size in the field\n",
    "- Honeybee: bees/m2/min Honeybee density in the field\n",
    "- Bumbles: bees/m2/min Bumblebee density in the field\n",
    "- Andrena: bees/m2/min Andrena bee density in the field\n",
    "- Osmia: bees/m2/min Osmia bee density in the field\n",
    "- MaxOfUpperTRange: ℃ The highest record of the upper band daily air temperature during the bloom season\n",
    "- MinOfUpperTRange: ℃ The lowest record of the upper band daily air temperature\n",
    "- AverageOfUpperTRange: ℃ The average of the upper band daily air temperature\n",
    "- MaxOfLowerTRange: ℃ The highest record of the lower band daily air temperature\n",
    "- MinOfLowerTRange: ℃ The lowest record of the lower band daily air temperature\n",
    "- AverageOfLowerTRange: ℃ The average of the lower band daily air temperature\n",
    "- RainingDays: Day The total number of days during the bloom season, each of which has precipitation larger than zero\n",
    "- AverageRainingDays: Day The average of raining days of the entire bloom season\n",
    "\n",
    "**Resources**:\n",
    "- [Kaggle Challenge](https://www.kaggle.com/competitions/playground-series-s3e14/overview)\n",
    "- [Dataset](https://www.kaggle.com/datasets/shashwatwork/wild-blueberry-yield-prediction-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8a7b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import mlflow\n",
    "\n",
    "from pathlib import Path\n",
    "from colorama import Style, Fore\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, LearningCurveDisplay, learning_curve, ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, \\\n",
    "                            mean_absolute_percentage_error, r2_score\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4732f7df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Seaborn theme parameters\n",
    "theme_parameters =  {\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.top': False,\n",
    "    'grid.alpha':0.3,\n",
    "    'figure.figsize': (16, 6),\n",
    "    'font.family': 'Andale Mono',\n",
    "    'axes.titlesize': 24,\n",
    "    'figure.facecolor': '#E5E8E8',\n",
    "    'axes.facecolor': '#E5E8E8'\n",
    "}\n",
    "\n",
    "# Set the theme\n",
    "sns.set_theme(style='whitegrid',\n",
    "              palette=sns.color_palette('deep'), \n",
    "              rc=theme_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec051d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Colors\n",
    "black = Style.BRIGHT + Fore.BLACK\n",
    "magenta = Style.BRIGHT + Fore.MAGENTA\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blue = Style.BRIGHT + Fore.BLUE\n",
    "reset_colors = Style.RESET_ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53074ae2",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1600a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Switch flag for Kaggle Cloud\n",
    "kaggle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8601ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read training data\n",
    "if kaggle:\n",
    "    \n",
    "    # Read data from Kaggle FS\n",
    "    train_data = pd.read_csv('/kaggle/input/playground-series-s3e14/train.csv')\n",
    "    test_data = pd.read_csv('/kaggle/input/playground-series-s3e14/test.csv')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Define local data file paths\n",
    "    train_data_file_path = Path(os.path.abspath('')).parents[1] / 'data' / 'S3E14' / 'wild_blueberry_yield_train.csv'\n",
    "    test_data_file_path = Path(os.path.abspath('')).parents[1] / 'data' / 'S3E14' / 'wild_blueberry_yield_test.csv'\n",
    "   \n",
    "    train_data = pd.read_csv(train_data_file_path)\n",
    "    test_data = pd.read_csv(test_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459ebe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c761aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30353f72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca776a0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Shapes Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04feb9a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print shapes information\n",
    "print(f'{blue}Data Shapes:'\n",
    "      f'{blue}\\n- Train Data  -> {red}{train_data.shape}'\n",
    "      f'{blue}\\n- Test Data   -> {red}{test_data.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f29a883",
   "metadata": {},
   "source": [
    "## Null Values Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75508dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print null values information\n",
    "print(f'{blue}Data Null Values:'\n",
    "      f'{blue}\\n- Train Data  -> {red}{train_data.isnull().any().sum()}'\n",
    "      f'{blue}\\n- Test Data   -> {red}{test_data.isnull().any().sum()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c13e3",
   "metadata": {},
   "source": [
    "## Train vs Test Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13510e56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the KDE of each feature\n",
    "figure, ax = plt.subplots(4, 4, figsize=(16, 12))\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Fetch the data to plot (exclude the 'id' and 'label' columns)\n",
    "for index, column_name in enumerate(train_data.columns[1:-1]):\n",
    "    \n",
    "    # Plot data\n",
    "    sns.kdeplot(data=train_data[column_name],\n",
    "                label='Train',\n",
    "                ax=ax[index])\n",
    "    \n",
    "    sns.kdeplot(data=test_data[column_name],\n",
    "                label='Test',\n",
    "                ax=ax[index])\n",
    "    \n",
    "    ax[index].set_title(column_name, fontsize=14)\n",
    "    \n",
    "    ax[index].tick_params(labelrotation=45)\n",
    "    \n",
    "    # Retrieve legend information\n",
    "    handles = ax[index].get_legend_handles_labels()[0]\n",
    "    labels = ax[index].get_legend_handles_labels()[1]\n",
    "    ax[index].legend().remove()\n",
    "\n",
    "# Set the legend\n",
    "figure.legend(handles, \n",
    "              labels, \n",
    "              loc='upper center', \n",
    "              bbox_to_anchor=(0.5, 1.03), \n",
    "              fontsize=12,\n",
    "              ncol=3)\n",
    "\n",
    "figure.suptitle('Train vs Test Feature Distribution',\n",
    "                fontweight='bold',\n",
    "                fontsize=24)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95469732",
   "metadata": {},
   "source": [
    "### Honeybee Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba0a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define figure and axes\n",
    "figure, ax = plt.subplots(2, 1, figsize=(12, 6))\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Plot the Boxplot of 'honeybee'\n",
    "sns.boxplot(data=train_data,\n",
    "            x='honeybee',\n",
    "            ax=ax[0])\n",
    "\n",
    "sns.boxplot(data=test_data,\n",
    "            x='honeybee',\n",
    "            ax=ax[1])\n",
    "\n",
    "ax[0].set_title('Train Honeybee Distribution')\n",
    "ax[1].set_title('Test Honeybee Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5933f04f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define figure and axes\n",
    "figure, ax = plt.subplots(2, 1, figsize=(12, 6))\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Plot the KDEs of 'honeybee'\n",
    "sns.kdeplot(data=train_data[train_data['honeybee'] < 2.5]['honeybee'],\n",
    "            label='Train',\n",
    "            ax=ax[0])\n",
    "\n",
    "sns.kdeplot(data=test_data[test_data['honeybee'] < 2.5]['honeybee'],\n",
    "            label='Test',\n",
    "            color='orange',\n",
    "            ax=ax[1])\n",
    "\n",
    "figure.suptitle('Honeybee KDE', fontsize=24)\n",
    "\n",
    "figure.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53069f8a",
   "metadata": {},
   "source": [
    "### Drop Outliers Honeybee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77455937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop outlisers in 'honeybee'\n",
    "train_data = train_data[train_data['honeybee'] < 2.5]\n",
    "test_data = test_data[test_data['honeybee'] < 2.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc91b7",
   "metadata": {},
   "source": [
    "- Dropped outlisers in `honeybee`\n",
    "- Exclude `honeybee`, `bumbles`, `andrena` and `osmia` from Standard Scaler\n",
    "- No differences between Train and Test Feature Distributions\n",
    "- Temperatures have the exact same distribution\n",
    "- Fruits and seeds information have almost the same distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27cc898",
   "metadata": {},
   "source": [
    "## Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e2426e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the KDE of 'yield'\n",
    "sns.kdeplot(data=train_data['yield'])\n",
    "\n",
    "plt.title('Yield KDE', fontsize=24)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b65f68",
   "metadata": {},
   "source": [
    "### Yield vs Fruit Set vs Fruit Mass vs Seeds KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f212fdcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define figure and axes\n",
    "figure, ax = plt.subplots(4, 1, figsize=(16, 9))\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Plot the KDEs of 'honeybee'\n",
    "sns.kdeplot(data=train_data['yield'], \n",
    "            label='yield', \n",
    "            ax=ax[0])\n",
    "\n",
    "sns.kdeplot(data=train_data['fruitset'], \n",
    "            label='fuitset',  \n",
    "            ax=ax[1])\n",
    "\n",
    "sns.kdeplot(data=train_data['fruitmass'], \n",
    "            label='fruitmass', \n",
    "            ax=ax[2])\n",
    "\n",
    "sns.kdeplot(data=train_data['seeds'], \n",
    "            label='seeds',\n",
    "            ax=ax[3])\n",
    "\n",
    "figure.suptitle('Yield vs Fruit Set vs Fruit Mass vs Seeds KDEs', fontsize=24)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87bfc5",
   "metadata": {},
   "source": [
    "The following features have almost the same distribution as the label:\n",
    "- `fruitset`    \n",
    "- `fruitmass`\n",
    "- `seeds`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7436cb",
   "metadata": {},
   "source": [
    "## Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48354ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "correlation_matrix = train_data.iloc[:, 1:].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f89ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a mask for the upper triangle\n",
    "correlation_mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dbe5e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define figure and axis\n",
    "figure, ax = plt.subplots(figsize=(30, 12))\n",
    "\n",
    "# Plot the correlation matrix\n",
    "sns.heatmap(correlation_matrix, \n",
    "            mask=correlation_mask, \n",
    "            cmap='mako',\n",
    "            vmax=1.0, \n",
    "            vmin=-1.0, \n",
    "            center=0, \n",
    "            square=True, \n",
    "            linewidths=.5, \n",
    "            annot=True,\n",
    "            annot_kws={'fontsize': 10},\n",
    "            cbar_kws={\"shrink\":.8, 'orientation':'vertical'})\n",
    "\n",
    "# Set title\n",
    "ax.set_title('Pearson Correlation', \n",
    "             fontsize=24, \n",
    "             fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223279ea",
   "metadata": {},
   "source": [
    "- There an important correlation between `clonesize` and `honeybee`\n",
    "- There is another confirmation that`fruitset`, `fruitmass` and `seeds` are strongly from `yield`. However, the strong correlation and the almost same distribution might mean that those three features are retrieved in the exact same moment in which also the yield is retrieved. They might be not available in a previous time and thus not useful in case of a previous prediction time.\n",
    "- All the temperatures are duplicated features. Just keep `MaxOfUpperTRange`.\n",
    "- The features `AverageRainingDays` and `RainingDays` are identical. Keep only `AverageRainingDays`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423f5a79",
   "metadata": {},
   "source": [
    "## Pairplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51be23e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the Pairplot between features and the target\n",
    "sns.pairplot(train_data[['clonesize', 'honeybee', 'bumbles', 'andrena', 'osmia', 'MaxOfUpperTRange', 'AverageRainingDays', 'fruitset', 'fruitmass', 'seeds', 'yield']],\n",
    "             kind=\"reg\",\n",
    "             diag_kind='kde',\n",
    "             plot_kws={'line_kws':{'color':'red'}},\n",
    "             corner=True)\n",
    "\n",
    "# Set title plot\n",
    "plt.suptitle('Features and Target Pairplots', \n",
    "             fontsize=20, \n",
    "             fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "406a2cb7",
   "metadata": {},
   "source": [
    "There are no non-linear correlations that have not emerged from the Pearson Correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db133741",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "- No null values\n",
    "- Dropped outlisers in `honeybee`\n",
    "- No differences between Train and Test Feature Distributions\n",
    "- The following features have almost the same distribution as the label:\n",
    "    - `fruitset`    \n",
    "    - `fruitmass`\n",
    "    - `seeds`\n",
    "- There is an important correlation between `clonesize` and `honeybee`\n",
    "- All the temperatures are duplicated features. Just keep `MaxOfUpperTRange`.\n",
    "- The features `AverageRainingDays` and `RainingDays` are identical. Keep only `AverageRainingDays`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884802e8",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a8436",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a0779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_engineered_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a pre-defined set of engineered feature to the input DataFrame\n",
    "    \n",
    "    Args:\n",
    "        data Pandas.DataFrame input\n",
    "    \n",
    "    Returns:\n",
    "        data Pandas.DataFrame with additional engineered features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a feature `fruitset per fruitmass`\n",
    "    data['fruitset per fruitmass'] = data['fruitset'] * data['fruitmass']\n",
    "    \n",
    "    # Create a feature `fruitset per seeds`\n",
    "    data['fruitset per seeds'] = data['fruitset'] * data['seeds']\n",
    "    \n",
    "    # Create a feature `fruitmass per seeds`\n",
    "    data['fruitmass per seeds'] = data['fruitmass'] * data['seeds']\n",
    "    \n",
    "    # Create a feature `clonesize per honeybee`\n",
    "    data['clonesize per honeybee'] = data['clonesize'] * data['honeybee']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34468a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the feature engineering\n",
    "train_data_engineered = compute_engineered_features(train_data)\n",
    "test_data_engineered = compute_engineered_features(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f29e0e",
   "metadata": {},
   "source": [
    "## Features and Labels Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5431de5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define features and labels\n",
    "numerical_features = ['clonesize', \n",
    "                      'honeybee', \n",
    "                      'bumbles', \n",
    "                      'andrena', \n",
    "                      'osmia', \n",
    "                      'MaxOfUpperTRange', \n",
    "                      'AverageRainingDays', \n",
    "                      'fruitset', \n",
    "                      'fruitmass', \n",
    "                      'seeds']\n",
    "\n",
    "numerical_engineered_featuers = ['fruitset per fruitmass',\n",
    "                                 'fruitset per seeds', \n",
    "                                 'fruitmass per seeds', \n",
    "                                 'clonesize per honeybee']\n",
    "\n",
    "categorical_features = []\n",
    "\n",
    "categorical_engineered_features = []\n",
    "\n",
    "features = numerical_features + numerical_engineered_featuers + categorical_features + categorical_engineered_features\n",
    "\n",
    "labels = ['yield']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bca2ba",
   "metadata": {},
   "source": [
    "## Numerical Features Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccdc5e1-7e44-4061-88ec-e26003d9ca7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop outliers from the training data\n",
    "train_data_engineered = train_data_engineered[train_data_engineered['honeybee'] < 2.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886676b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Numerical features pipeline\n",
    "numerical_features_pipeline = Pipeline(steps=[\n",
    "    ('numerical_scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff6944c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bundle Data Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db571e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bunlde data preprocessing steps\n",
    "data_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numerical_preprocessing', numerical_features_pipeline, numerical_features + numerical_engineered_featuers)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158800c",
   "metadata": {},
   "source": [
    "# Train & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddab4be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define X and y for the training set\n",
    "X = train_data_engineered[features]\n",
    "y = train_data_engineered[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8459de1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split training data into train and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=108)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d47cb9",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f1ee44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set MLflow Experiment\n",
    "mlflow_experiment_name = 'Wild Blueberry Yield'\n",
    "\n",
    "# Create experiment or retrieve already existing experiment\n",
    "try:\n",
    "    mlflow_experiment_id = mlflow.create_experiment(name=mlflow_experiment_name)\n",
    "except Exception as e:\n",
    "    mlflow_experiment_id = mlflow.get_experiment_by_name(mlflow_experiment_name).experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ac082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define trained models\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f98c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the used metrics\n",
    "metrics = ['RMSE', 'MSE', 'MAE', 'MAPE', 'R2 Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f62d5c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize DataFrame of model performance\n",
    "performance = pd.DataFrame(columns=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063673bd",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c0799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Start MLflow Run\n",
    "with mlflow.start_run(experiment_id=mlflow_experiment_id, \n",
    "                      run_name='Linear Regression'):\n",
    "    # Define the model\n",
    "    model_lr = LinearRegression()\n",
    "\n",
    "    # Define the pipeline\n",
    "    pipe_lr = Pipeline([\n",
    "        ('data_preprocessing', data_preprocessor),\n",
    "        ('linear_regression', model_lr)\n",
    "    ])\n",
    "    \n",
    "    # Train the pipeline\n",
    "    pipe_lr.fit(X_train, \n",
    "                y_train)\n",
    "\n",
    "    # Get predictions\n",
    "    predictions_lr = pipe_lr.predict(X_test)\n",
    "\n",
    "    # Compute metrics\n",
    "    rmse_lr = round(mean_squared_error(y_test, predictions_lr) ** 0.5, 2)\n",
    "    mse_lr = round(mean_squared_error(y_test, predictions_lr), 2)\n",
    "    mae_lr = round(mean_absolute_error(y_test, predictions_lr), 2)\n",
    "    mape_lr = round(mean_absolute_percentage_error(y_test, predictions_lr), 2)\n",
    "    r2_score_lr = round(r2_score(y_test, predictions_lr), 2)\n",
    "\n",
    "    print('RMSE: {}'.format(rmse_lr))\n",
    "    print('MSE: {}'.format(mse_lr))\n",
    "    print('MAE: {}'.format(mae_lr))\n",
    "    print('MAPE: {}'.format(mape_lr))\n",
    "    print('R2 Score: {}'.format(r2_score_lr))\n",
    "    print('\\n')\n",
    "    \n",
    "    # Log model's evaluation metrics\n",
    "    mlflow.log_metrics({'RMSE': rmse_lr, \n",
    "                        'MSE': mse_lr, \n",
    "                        'MAE': mae_lr, \n",
    "                        'MAPE': mape_lr, \n",
    "                        'R2': r2_score_lr})\n",
    "    \n",
    "    # Log model's features\n",
    "    mlflow.log_params({'Features': features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002dc175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update 'performance' DataFrame\n",
    "performance.loc['Logistic Regression'] = [rmse_lr, mse_lr, mae_lr, mape_lr, r2_score_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbfd0bc-d0bc-43c4-ad13-c9957e7db250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add the trained model to the list of models\n",
    "models['Linear Regression'] = pipe_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847bd736",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf48fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XGBoost Hyperparameters\n",
    "hyperparameter_xgb = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d9eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Start MLflow Run\n",
    "with mlflow.start_run(experiment_id=mlflow_experiment_id, \n",
    "                      run_name='XGBoost'):\n",
    "    # Define the model\n",
    "    model_xgb = XGBRegressor(**hyperparameter_xgb)\n",
    "\n",
    "    # Define the pipeline\n",
    "    pipe_xgb = Pipeline([\n",
    "        ('data_preprocessing', data_preprocessor),\n",
    "        ('xgboost', model_xgb)\n",
    "    ])\n",
    "\n",
    "    # Train the pipeline\n",
    "    pipe_xgb.fit(X_train, \n",
    "                y_train)\n",
    "\n",
    "    # Get predictions\n",
    "    predictions_xgb = pipe_xgb.predict(X_test)\n",
    "\n",
    "    # Compute metrics\n",
    "    rmse_xgb = round(mean_squared_error(y_test, predictions_xgb) ** 0.5, 2)\n",
    "    mse_xgb = round(mean_squared_error(y_test, predictions_xgb), 2)\n",
    "    mae_xgb = round(mean_absolute_error(y_test, predictions_xgb), 2)\n",
    "    mape_xgb = round(mean_absolute_percentage_error(y_test, predictions_xgb), 2)\n",
    "    r2_score_xgb = round(r2_score(y_test, predictions_xgb), 2)\n",
    "\n",
    "    print('RMSE: {}'.format(rmse_xgb))\n",
    "    print('MSE: {}'.format(mse_xgb))\n",
    "    print('MAE: {}'.format(mae_xgb))\n",
    "    print('MAPE: {}'.format(mape_xgb))\n",
    "    print('R2 Score: {}'.format(r2_score_xgb))\n",
    "    print('\\n')\n",
    "    \n",
    "    # Log model's evaluation metrics\n",
    "    mlflow.log_metrics({'RMSE': rmse_xgb, \n",
    "                        'MSE': mse_xgb, \n",
    "                        'MAE': mae_xgb, \n",
    "                        'MAPE': mape_xgb, \n",
    "                        'R2': r2_score_xgb})\n",
    "    \n",
    "    # Log model's features\n",
    "    mlflow.log_params({'Features': features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fc629c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update 'performance' DataFrame\n",
    "performance.loc['XGBoost'] = [rmse_xgb, mse_xgb, mae_xgb, mape_xgb, r2_score_xgb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5abe68-ac22-4f1f-bb3b-2ab5a4ed6df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add the trained model to the list of models\n",
    "models['XGBoost'] = pipe_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bfe983",
   "metadata": {},
   "source": [
    "# Model Explanability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57340dac",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06f1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort dataframe by the metric\n",
    "performance.sort_values('RMSE', inplace=True)\n",
    "\n",
    "# Create figure\n",
    "figure = plt.figure(tight_layout=True, figsize=(9, 6))\n",
    "\n",
    "# Plot models' metrics\n",
    "ax = sns.barplot(data=performance, \n",
    "                 x=performance.index.tolist(), \n",
    "                 y='RMSE')\n",
    "\n",
    "# Set title\n",
    "ax.set_title('Models Comparison', \n",
    "             fontsize=24)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df9355c-7810-4793-b679-8f76b5a056ea",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df91e71b-b86d-4f15-b260-64be9ff7cd6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define figure and axes\n",
    "figure, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Fetch all the trained models\n",
    "for index, model_name in enumerate(models.keys()):\n",
    "    \n",
    "    # Retrieve feature names\n",
    "    feature_names = models[model_name].feature_names_in_\n",
    "    \n",
    "    # Retrieve pipeline model's step name\n",
    "    pipe_model_step_name = list(models[model_name].named_steps.keys())[-1]\n",
    "    \n",
    "    # Retrieve feature importances values\n",
    "    try:\n",
    "        # ree-based models\n",
    "        feautre_importance_values = models[model_name].named_steps[pipe_model_step_name].feature_importances_\n",
    "    except:\n",
    "        # Regression-based models\n",
    "        feautre_importance_values = models[model_name].named_steps[pipe_model_step_name].coef_.reshape(-1,)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Compute the feature importance\n",
    "    feature_importance = sorted(list(zip(feature_names,\n",
    "                                         feautre_importance_values)),\n",
    "                                key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Transform it into a DataFrame\n",
    "    feature_importance_df = pd.DataFrame(feature_importance,\n",
    "                                         columns= ['Feature', 'Importance'])\n",
    "    \n",
    "    break\n",
    "    \n",
    "    \n",
    "    # Plot the feature importance\n",
    "    sns.barplot(data=feature_importance_df,\n",
    "               x='Feature',\n",
    "               y='Importance', \n",
    "               ax=ax[index])\n",
    "\n",
    "    # Set title\n",
    "    ax[index].set_title('Feature Importance', \n",
    "                 fontsize=20, \n",
    "                 fontweight='bold')\n",
    "\n",
    "    ax[index].xticks(fontsize=8, \n",
    "               rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362997a3-e6b1-41ed-94bf-0c572c84f7be",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f85da55-b242-4e4c-a2d0-128508ad2cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Learning Curves Display parameters\n",
    "learning_curves_display_paramters = {\n",
    "    'X': X,\n",
    "    'y': y,\n",
    "    'cv': ShuffleSplit(n_splits=10, test_size=0.3, random_state=108),\n",
    "    'train_sizes': np.linspace(0.1, 1.0, 10),\n",
    "    'scoring': 'neg_root_mean_squared_error',\n",
    "    'negate_score': True,\n",
    "    'score_name': 'RMSE',\n",
    "    'score_type': 'both',\n",
    "    'verbose': 0,\n",
    "    'line_kw': {'marker': 'o'},\n",
    "    'std_display_style': 'fill_between',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7e4e2-b815-4927-b74f-36c4f18440e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define figure and axes\n",
    "figure, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Fetch all the trained models\n",
    "for index, model_name in enumerate(models.keys()):\n",
    "\n",
    "    # Plot the Learning Curve\n",
    "    LearningCurveDisplay.from_estimator(models[model_name], \n",
    "                                        **learning_curves_display_paramters,\n",
    "                                        ax=ax[index])\n",
    "    \n",
    "    # Retrieve legend information\n",
    "    handles = ax[index].get_legend_handles_labels()[0]\n",
    "    labels = ax[index].get_legend_handles_labels()[1]\n",
    "    ax[index].legend().remove()\n",
    "    \n",
    "     # Set the title\n",
    "    ax[index].set_title(model_name, fontsize=14)\n",
    "    \n",
    "# Set the legend\n",
    "figure.legend(handles, \n",
    "              labels, \n",
    "              loc='upper center', \n",
    "              bbox_to_anchor=(0.5, 1.03), \n",
    "              fontsize=8,\n",
    "              ncol=2)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e99c1-5a26-4937-9529-a5d6abf0b00c",
   "metadata": {},
   "source": [
    "From the Learning Curves it is possible to see that XGBoost is learning better from more data with respect to Linear Regression, which seems to be quite randomic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8400e6-1d06-4f78-a345-1a36e1d2e394",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a2a6c2-4190-4d74-9c13-fc3e34ad925a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define figure and axes\n",
    "figure, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax = ax.flatten()\n",
    "\n",
    "# Fetch all the trained models\n",
    "for index, model_name in enumerate(models.keys()):\n",
    "    \n",
    "    # Compute the predictions for the test set\n",
    "    predictions = models[model_name].predict(X_test)\n",
    "    \n",
    "    print(predictions\n",
    "    \n",
    "    # Compute the residuals\n",
    "    residuals = y_test - predictions\n",
    "    \n",
    "    # Create the Pandas DataFrame\n",
    "    residuals_df = pd.DataFrame({'Residuals': residuals.to_numpy().reshape(-1,), \n",
    "                                 'Predictions': predictions.reshape(-1,)})\n",
    "    \n",
    "    # Plot the residuals\n",
    "    sns.residplot(x='Predictions', \n",
    "                  y='Residuals',\n",
    "                  data=residuals_df, \n",
    "                  ax=ax[index],\n",
    "                  lowess=True)\n",
    "\n",
    "    plt.title('Gradient Boosting Regressor Feature Importance Residual plot')\n",
    "\n",
    "    # Retrieve legend information\n",
    "    handles = ax[index].get_legend_handles_labels()[0]\n",
    "    labels = ax[index].get_legend_handles_labels()[1]\n",
    "    ax[index].legend().remove()\n",
    "    \n",
    "     # Set the title\n",
    "    ax[index].set_title(model_name, fontsize=14)\n",
    "\n",
    "# Set the legend\n",
    "figure.legend(handles, \n",
    "              labels, \n",
    "              loc='upper center', \n",
    "              bbox_to_anchor=(0.5, 1.03), \n",
    "              fontsize=8,\n",
    "              ncol=2)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32c000-fd8b-43c2-a4ba-2387c5ebb6d3",
   "metadata": {},
   "source": [
    "## Q-Q Plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
