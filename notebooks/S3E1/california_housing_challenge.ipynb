{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# California Housing Challenge\n\nThe notebook is intended to predict the average house value upon the provided house features.","metadata":{}},{"cell_type":"code","source":"# Import Standard Libraries\nimport pandas as pd\nimport numpy as np\nimport pprint\n\nfrom scipy.stats import zscore\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nimport mlflow\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, KFold, RepeatedKFold, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nfrom xgboost import XGBRegressor\n\nimport lightgbm as lgb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define Seaborn theme parameters\ntheme_parameters =  {\n    'axes.spines.right': False,\n    'axes.spines.top': False,\n    'grid.alpha':0.3,\n    'figure.figsize': (16, 6),\n    'font.family': 'Andale Mono',\n    'axes.titlesize': 24,\n    'figure.facecolor': '#E5E8E8',\n    'axes.facecolor': '#E5E8E8'\n}\n\n# Set the theme\nsns.set_theme(style='whitegrid',\n              palette=sns.color_palette('deep'), \n              rc=theme_parameters)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read Data","metadata":{}},{"cell_type":"code","source":"# Switch flag for Kaggle Cloud\nkaggle = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read training data\ntrain_data = pd.read_csv('./../../data/S3E1/california_housing_train.csv')\ntest_data = pd.read_csv('./../../data/S3E1/california_housing_test.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"markdown","source":"## Train Features & Label Distribution","metadata":{"tags":[]}},{"cell_type":"code","source":"# Plot the histograms of each feature\nfigure, ax = plt.subplots(3, 3, figsize=(16, 9))\nax = ax.flatten()\n\n# Fetch the data to plot (exclude the 'id' column)\nfor index, column_name in enumerate(train_data.columns[1:]):\n\n    # Plot data\n    sns.histplot(data=train_data[column_name], \n                 ax=ax[index])\n\n    ax[index].set_title(column_name, \n                        fontsize=14, \n                        fontweight='bold')\n\n    ax[index].tick_params(labelrotation=45)\n\nplt.suptitle('Feature & Label Distrubtion', \n             fontweight='bold',\n             fontsize=30)\n\nplt.tight_layout()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Average Occupancy Distribution","metadata":{}},{"cell_type":"code","source":"# Plot the histogram of 'AveOccup'\nax = sns.boxplot(data=train_data, \n                  x='AveOccup')\n\n\nax.set_title('Average Occupancy Distribution')\n\nplt.tight_layout()\n\nplt.show()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the outlier\ntrain_data = train_data[train_data['AveOccup'] < 100].reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `HouseAge` has some strange peaks. Probably some rounding operations\n- `AveOccup` has a huge outlier. It has been dropped for EDA sake.\n- `MedHouseVal` has a strange peak at the end. Probably a cap","metadata":{}},{"cell_type":"markdown","source":"## Pearson Correlation","metadata":{}},{"cell_type":"code","source":"# Compute the correlation matrix\ncorrelation_matrix = train_data.iloc[:, 1:].corr()\n\n# Generate a mask for the upper triangle\ncorrelation_mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n\n# Define figure and axis\nfigure, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the correlation matrix\nsns.heatmap(correlation_matrix, \n            mask=correlation_mask, \n            cmap='mako',\n            vmax=1.0, \n            vmin=-1.0, \n            center=0, \n            square=True, \n            linewidths=.5, \n            annot=True,\n            annot_kws={'fontsize': 8},\n            cbar_kws={\"shrink\":.8, 'orientation':'vertical'})\n\n# Set title\nax.set_title('Pearson Correlation', \n             fontsize=20, \n             fontweight='bold')\n\nplt.tight_layout()\n\nplt.show()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Positive correlation between:\n- `AveRooms` and `MedInc`","metadata":{}},{"cell_type":"markdown","source":"## Train Data Geography","metadata":{}},{"cell_type":"code","source":"# Define dots colors\ncolor_scale = [(0, 'orange'), (1,'red')]\n\n# Plot the data\nfigure = px.scatter_mapbox(train_data,\n                           lat=\"Latitude\",\n                           lon=\"Longitude\",\n                           hover_name=\"MedHouseVal\",\n                           hover_data=[\"MedHouseVal\"],\n                           color=\"MedHouseVal\",\n                           color_continuous_scale=color_scale,\n                           size=\"MedHouseVal\",\n                           zoom=8,\n                           height=600,\n                           width=600)\n\nfigure.update_layout(mapbox_style=\"open-street-map\")\nfigure.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfigure.show()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define an `hot_area`flag for the houses sold in the following squares:\n- (-123 <= lon <= -121.45) and (36.6 <= lat <= 38.6)\n- (-119.90 <= lon <= -119.42) and (34.25 <= lat <= 34.5)\n- (-119.35 <= lon <= -117.55) and (33.36 <= lat <= 34.52)\n- (-117.29 <= lon <= -117.18) and (32.84 <= lat <= 33.01)","metadata":{}},{"cell_type":"markdown","source":"## Train vs Test Feature & Label Distribution","metadata":{}},{"cell_type":"code","source":"# Plot the KDE of each feature\nfigure, ax = plt.subplots(3, 3, figsize=(16, 12))\nax = ax.flatten()\n\n# Fetch the data to plot (exclude the 'id' and 'quality' columns)\nfor index, column_name in enumerate(train_data.columns[1:-1]):\n    \n    # Plot data\n    sns.kdeplot(data=train_data[column_name],\n                label='Train',\n                ax=ax[index])\n    \n    sns.kdeplot(data=test_data[column_name],\n                label='Test',\n                ax=ax[index])\n    \n    ax[index].set_title(column_name, fontsize=14)\n    \n    ax[index].tick_params(labelrotation=45)\n    \n    # Retrieve legend information\n    handles = ax[index].get_legend_handles_labels()[0]\n    labels = ax[index].get_legend_handles_labels()[1]\n    ax[index].legend().remove()\n    \n# Remove the empty subplot\nfigure.delaxes(ax[-1])\n\n# Set the legend\nfigure.legend(handles, \n              labels, \n              loc='upper center', \n              bbox_to_anchor=(0.5, 1.03), \n              fontsize=12,\n              ncol=2)\n\nplt.tight_layout()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No difference in the train and test data distributions.","metadata":{}},{"cell_type":"markdown","source":"## Count Outliers with the Z-Score","metadata":{}},{"cell_type":"code","source":"# Compute the Z-Score for the feature columns across 'quality' classes\nz_scores = train_data.iloc[:, 1:-1].apply(zscore)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Consider as an 'outlier' every records with a Z-Score bigger than 2 SDs in absolute value terms\noutliers = z_scores.abs().ge(2).sum().to_frame('Count').sort_values(by='Count')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the feature importance\nax = sns.barplot(data=outliers, \n                 x=outliers.index.tolist(), \n                 y='Count')\n\n# Set title\nax.set_title('Outlisers Count', \n             fontsize=20, \n             fontweight='bold')\n\nplt.xticks(fontsize=12, \n           rotation=45)\n\nplt.show()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `HouseAge` has confirmed to have that strange spike at the end. It is necessary to create interval of classes for this feature.","metadata":{}},{"cell_type":"markdown","source":"## Train Features Pairplot","metadata":{}},{"cell_type":"code","source":"# Plot the Pairplot between the features\nsns.pairplot(train_data.drop(columns=['id', 'MedHouseVal']),\n             kind=\"reg\",\n             diag_kind='kde',\n             plot_kws={'line_kws':{'color':'red'}},\n             corner=True)\n\n# Set title plot\nplt.suptitle('Train Feature Pairplot', \n             fontsize=20, \n             fontweight='bold')\n\nplt.tight_layout()\n\nplt.show()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Positive correlations:\n- `AveRooms` and `MedInc`\n- `AveBedrms` and `AveRooms`","metadata":{}},{"cell_type":"markdown","source":"## Conclusions","metadata":{}},{"cell_type":"markdown","source":"- Round the HouseAge to 3 classes\n- Create a feature `AveRooms per MedInc`\n- Create a feature `HotArea`\n- Create a feature `AveBedrms per AveRooms`","metadata":{}},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"def compute_engineered_features(data: pd.DataFrame) -> pd.DataFrame:\n    \n    \"\"\"\n    Create a pre-defined set of engineered feature to the input DataFrame\n    \n    Args:\n        data Pandas.DataFrame input\n    \n    Returns:\n        data Pandas.DataFrame with additional engineered features\n    \"\"\"\n    \n    # Define the conditions for the `HouseAgeClass` categories\n    house_age_class_conditions = {\n        1: data['HouseAge'] <= 17,\n        2: (data['HouseAge'] > 17) &  (data['HouseAge'] < 52),\n        3: data['HouseAge'] == 52\n    }\n\n    # Define a categorical variable called `HouseAgeClass`\n    data['HouseAgeClass'] = np.select(house_age_class_conditions.values(),\n                                      house_age_class_conditions.keys())\n    \n    \n    # Create a feature `AveRooms per MedInc`\n    data['AveRooms per MedInc'] = data['AveRooms'] * data['MedInc']\n    \n    # Create a feature `HotArea`\n    # Define the rectangular areas of interest\n    hot_areas = [(-123, -121.45, 36.6, 38.6),\n                 (-119.90, -119.42, 34.25, 34.5),\n                 (-119.35, -117.55, 33.86, 34.52),\n                 (-117.29, -117.18, 32.84, 33.01)]\n\n    # Check if each point is inside any of the hot areas\n    is_in_hot_area = False\n    for area in hot_areas:\n        is_in_hot_area |= ((data['Longitude'] >= area[0]) &\n                           (data['Longitude'] <= area[1]) &\n                           (data['Latitude'] >= area[2]) &\n                           (data['Latitude'] <= area[3]))\n        \n    # Assign a binary value to indicate if a point is in a hot area or not\n    data['HotArea'] = np.where(is_in_hot_area, 1, 0)\n\n    \n    # Create a feature `AveBedrms per AveRooms`\n    data['AveBedrms per AveRooms'] = data['AveBedrms'] * data['AveRooms']\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the feature engineering\ncompute_engineered_features(train_data)\ncompute_engineered_features(test_data)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features and Labels Definition","metadata":{}},{"cell_type":"code","source":"# Define features and labels\nnumerical_features = train_data.columns[1:9].tolist()\n\nnumerical_engineered_featuers = ['AveRooms per MedInc', \n                                 'AveBedrms per AveRooms']\n\ncategorical_features = []\n\ncategorical_engineered_features = []\n                                   #'HouseAgeClass',\n                                   #'HotArea']\n\nfeatures = numerical_features + numerical_engineered_featuers + categorical_features + categorical_engineered_features\n\nlabels = ['MedHouseVal']","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Numerical Features Preprocessing Pipeline","metadata":{}},{"cell_type":"code","source":"# Numerical features pipeline\nnumerical_features_pipeline = Pipeline(steps=[\n    ('numerical_scaler', StandardScaler()) # TODO: Add filtering outliers\n])","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bundle Data Preprocessing Steps","metadata":{}},{"cell_type":"code","source":"# Bunlde data preprocessing steps\ndata_preprocessor = ColumnTransformer(\n    transformers=[\n        ('numerical_preprocessing', \n         numerical_features_pipeline, \n         numerical_features + numerical_engineered_featuers),\n        ('categorical_preprocessing', \n         'passthrough', \n         categorical_engineered_features)\n    ])","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train & Test Split","metadata":{}},{"cell_type":"code","source":"# Define X and y for the training set\nX = train_data[numerical_features + numerical_engineered_featuers + categorical_engineered_features]\ny = train_data[labels]","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split training data into train and validation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# Set MLflow Experiment\nmlflow_experiment_name = 'California Housing Price'\n\n# Create experiment or retrieve already existing experiment\ntry:\n    mlflow_experiment_id = mlflow.create_experiment(name=mlflow_experiment_name)\nexcept Exception as e:\n    mlflow_experiment_id = mlflow.get_experiment_by_name(mlflow_experiment_name).experiment_id","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the used metrics\nmetrics = ['RMSE']","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize DataFrame of model performance\nperformance = pd.DataFrame(columns=metrics)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Start MLflow Run\nwith mlflow.start_run(experiment_id=mlflow_experiment_id, \n                      run_name='Linear Regression'):\n    # Define the model\n    model_lr = LinearRegression()\n\n    # Define the pipeline\n    pipe_lr = Pipeline([\n        ('data_preprocessing', data_preprocessor),\n        ('linear_regression', model_lr)\n    ])\n\n    # Train the pipeline\n    pipe_lr.fit(X_train, \n                y_train)\n\n    # Get predictions\n    predictions_lr = pipe_lr.predict(X_test)\n\n    # Compute metrics\n    rmse_lr = round(mean_squared_error(y_test,\n                                       predictions_lr), 2)\n\n    print('RMSE: {}'.format(rmse_lr))\n    print('\\n')\n    \n    # Log model's evaluation metrics\n    mlflow.log_metrics({'RMSE': rmse_lr})\n    \n    # Log model's features\n    mlflow.log_params({'Features': features})","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update 'performance' DataFrame\nperformance.loc['Logistic Regression'] = [rmse_lr]","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression - Cross-Validation","metadata":{}},{"cell_type":"code","source":"# Define the Cross-Validation Iterators\nkfold = KFold(n_splits=4)\nrepeated_kfold = RepeatedKFold(n_splits=4, n_repeats=2, random_state=42)","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_with_stratified_kfold(estimator, X, y, cv):\n    \"\"\"\n    Train an Estimator through a Stratified K-Fold Cross-Validation approach and return evaluation metrics\n    \n        Paramarameters:\n            estimator: sklearn.linear_model estimator to optimise\n            X: Pandas DataFrame of data\n            y: Pandas DataFrame of labels\n            cv: sklearn.model_selection splitter instance\n            \n        Returns:\n            rmse: Float RMSE\n    \"\"\"\n    \n    # Initialise empty lists for metrics\n    rmse_list = []\n    \n    # Fetch the folds\n    for fold, (train_index, validation_index) in enumerate(cv.split(X, y)):\n        \n        # Split the data\n        X_train = X.loc[train_index]\n        X_validation = X.loc[validation_index]\n        y_train = y.loc[train_index]\n        y_validation = y.loc[validation_index]\n        \n        # Fit the estimator\n        estimator.fit(X_train, y_train)\n        \n        # Get predictions\n        predictions = estimator.predict(X_validation)\n        \n        # Compute metrics\n        rmse_fold = round(mean_squared_error(y_validation, predictions), 2)\n        \n        print('---- Fold {} ----'.format(fold))\n        print('RMSE: {}'.format(rmse_fold))\n        print('\\n')\n        \n        # Append mentrics to the corresponding list\n        rmse_list.append(rmse_fold)\n        \n    # Compute metrics average\n    rmse = round(np.mean(rmse_list), 2)\n        \n    return rmse","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K-Fold","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Start MLflow Run\nwith mlflow.start_run(experiment_id=mlflow_experiment_id, \n                      run_name='Linear Regression K-Fold'):\n\n    # Define the model\n    model_lr_kfold = LinearRegression()\n\n    # Define the pipeline\n    pipe_lr_kfold = Pipeline([\n        ('data_preprocessing', data_preprocessor),\n        ('linear_regression', model_lr_kfold)\n    ])\n\n    # Train the pipeline with K-Fold\n    rmse_lr_kfold = train_with_stratified_kfold(pipe_lr_kfold, X, y, kfold)\n\n    print('RMSE: {}'.format(rmse_lr_kfold))\n    print('\\n')\n    \n    # Log model's evaluation metrics\n    mlflow.log_metrics({'RMSE': rmse_lr_kfold})\n    \n    # Log model's features\n    mlflow.log_params({'Features': features})\n\n    # Update 'performance' DataFrame\n    performance.loc['Logistic Regression - K-Fold'] = [rmse_lr_kfold]","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Repeated K-Fold","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Start MLflow Run\nwith mlflow.start_run(experiment_id=mlflow_experiment_id, \n                      run_name='Linear Regression Repeated K-Fold'):\n\n    # Define the model\n    model_lr_rkfold = LinearRegression()\n\n    # Define the pipeline\n    pipe_lr_rkfold = Pipeline([\n        ('data_preprocessing', data_preprocessor),\n        ('linear_regression', model_lr_rkfold)\n    ])\n\n    # Train the pipeline with K-Fold\n    rmse_lr_rkfold = train_with_stratified_kfold(pipe_lr_rkfold, X, y, repeated_kfold)\n\n    print('RMSE: {}'.format(rmse_lr_rkfold))\n    print('\\n')\n    \n    # Log model's evaluation metrics\n    mlflow.log_metrics({'RMSE': rmse_lr_rkfold})\n    \n    # Log model's features\n    mlflow.log_params({'Features': features})\n\n    # Update 'performance' DataFrame\n    performance.loc['Logistic Regression - Repeated K-Fold'] = [rmse_lr_rkfold]","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"# XGBoost Hyperparameters\nhyperparameter_xgb = {\n    'n_estimators': 500,\n    'max_depth': 5,\n    'learning_rate': 0.01\n}","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Start MLflow Run\nwith mlflow.start_run(experiment_id=mlflow_experiment_id, \n                      run_name='XGBoost'):\n\n    # Define the model\n    model_xgb = XGBRegressor(**hyperparameter_xgb)\n\n    # Define the pipeline\n    pipe_xgb = Pipeline([\n        ('data_preprocessing', data_preprocessor),\n        ('xgboost', model_xgb)\n    ])\n\n    # Train the pipeline\n    pipe_xgb.fit(X_train,\n                 y_train,\n                 xgboost__verbose=False)\n\n    # Get predictions\n    predictions_xgb = pipe_xgb.predict(X_test)\n\n    # Compute metrics\n    rmse_xgb = round(mean_squared_error(y_test, predictions_xgb), 2)\n    \n    # Log model's evaluation metrics\n    mlflow.log_metrics({'RMSE': rmse_xgb})\n    \n    # Log model's features\n    mlflow.log_params({'Features': features, 'Hyperparameters': hyperparameter_xgb})\n\n    print('RMSE: {}'.format(rmse_xgb))\n    print('\\n')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update 'performance' DataFrame\nperformance.loc['XGBoost'] = [rmse_xgb]","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM","metadata":{}},{"cell_type":"code","source":"# LightGBM hyperparameters\nhyperparameters_lgb = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'force_row_wise': True,\n    'n_estimators': 1000,\n    'verbose': 0\n}","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Start MLflow Run\nwith mlflow.start_run(experiment_id=mlflow_experiment_id, \n                      run_name='LightGBM'):\n\n    # Define the model\n    model_lgb = lgb.LGBMRegressor(**hyperparameters_lgb)\n\n    # Define the pipeline\n    pipe_lgb = Pipeline([\n        ('data_preprocessing', data_preprocessor),\n        ('lightgbm', model_lgb)\n    ])\n\n    # Train the pipeline\n    pipe_lgb.fit(X_train, \n                 np.ravel(y_train))\n\n    # Get predictions\n    predictions_lgb = pipe_lgb.predict(X_test)\n\n    # Compute metrics\n    rmse_lgb = round(mean_squared_error(y_test, predictions_lgb), 2)\n    \n    # Log model's evaluation metrics\n    mlflow.log_metrics({'RMSE': rmse_lgb})\n    \n    # Log model's features\n    mlflow.log_params({'Features': features, 'Hyperparameters': hyperparameters_lgb})\n\n    print('RMSE: {}'.format(rmse_lgb))\n    print('\\n')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update 'performance' DataFrame\nperformance.loc['LightGBM'] = [rmse_lgb]","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM with GridSearch","metadata":{}},{"cell_type":"code","source":"# LightGBM hyperparameters\nhyperparameters_lgb_gs = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse'\n}\n\n# LightGBM hyperparameters space\nhyperparameters_space_lgb_gs = {\n    'lightgbm__num_leaves': np.arange(10, 60, 10),\n    'lightgbm__learning_rate': np.linspace(0.001, 1, 5),\n    'lightgbm__n_estimators': np.arange(50, 1000, 100)\n}","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Start MLflow Run\nwith mlflow.start_run(experiment_id=mlflow_experiment_id, \n                      run_name='LightGBM with GridSearch'):\n\n    # Define the model\n    model_lgb_gs = lgb.LGBMRegressor(**hyperparameters_lgb_gs)\n\n    # Define the pipeline\n    pipe_lgb_gs = Pipeline([\n        ('data_preprocessing', data_preprocessor),\n        ('lightgbm', model_lgb_gs)\n    ])\n    \n    # Define GridSearch\n    # NOTE: GridSearchCV tries to maximise its score, that's why we need the Negative RMSE\n    grid_search_lgb_gs = GridSearchCV(estimator=pipe_lgb_gs, \n                                      param_grid=hyperparameters_space_lgb_gs, \n                                      cv=3, \n                                      n_jobs=2, \n                                      scoring='neg_root_mean_squared_error', \n                                      verbose=3)\n\n    # Search the best hyperparameters\n    grid_search_lgb_gs.fit(X_train, \n                           np.ravel(y_train))\n\n    # Retrieve the best model\n    best_model_lgb_gs = grid_search_lgb_gs.best_estimator_\n    \n    # Retrieve best model's parameters\n    best_parameters_lgb_gs = best_model_lgb_gs['lightgbm'].get_params()\n    \n    print(\"Model's Best Hyperparameters:\")\n    pprint.pprint(best_parameters_lgb_gs)\n    print('\\n')\n    \n    # Get predictions\n    predictions_lgb_gs = best_model_lgb_gs.predict(X_test)\n\n    # Compute metrics\n    rmse_lgb_gs = round(mean_squared_error(y_test, predictions_lgb_gs), 2)\n    \n    # Log model's evaluation metrics\n    mlflow.log_metrics({'RMSE': rmse_lgb_gs})\n    \n    # Log model's features\n    mlflow.log_params({'Features': features, 'Hyperparameters': best_parameters_lgb_gs})\n\n    print('RMSE: {}'.format(rmse_lgb_gs))\n    print('\\n')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models Comparison","metadata":{}},{"cell_type":"code","source":"# Sort dataframe by the metric\nperformance.sort_values('RMSE', inplace=True)\n\n# Plot models' metrics\nax = sns.barplot(data=performance, \n                 x=performance.index.tolist(), \n                 y='RMSE')\n\n# Set title\nax.set_title('Models Comparison', \n             fontsize=20, \n             fontweight='bold')\n\nplt.xticks(rotation=45)\n\nplt.show()","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Explanability","metadata":{}},{"cell_type":"code","source":"# Compute the feature importance\nfeature_importance = sorted(list(zip(pipe_lgb.feature_names_in_,\n                                     pipe_lgb['lightgbm'].feature_importances_)),\n                            key=lambda x: x[1], reverse=True)\n\n# Transform it into a DataFrame\nfeature_importance_df = pd.DataFrame(feature_importance,\n                                     columns= ['Feature', 'Importance'])","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the feature importance\nax = sns.barplot(data=feature_importance_df, \n                 x='Feature', \n                 y='Importance')\n\n# Set title\nax.set_title('Feature Importance', \n             fontsize=20, \n             fontweight='bold')\n\nplt.xticks(fontsize=8, \n           rotation=45)\n\nplt.show()","metadata":{"tags":[]},"execution_count":null,"outputs":[]}]}